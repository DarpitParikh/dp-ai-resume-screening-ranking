# -*- coding: utf-8 -*-
"""resume

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gc8IpBcyKpdonpOOi5FukVUc7culTrl-
"""

!pip install streamlit pyngrok scikit-learn nltk spacy PyPDF2 --quiet
!python -m spacy download en_core_web_sm

import streamlit as st
import PyPDF2
import nltk
import spacy
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
from pyngrok import ngrok

nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nlp = spacy.load("en_core_web_sm")

def extract_text_from_pdf(uploaded_file):
    """Extracts text from a PDF file."""
    text = ""
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        for page in pdf_reader.pages:
            text += page.extract_text() + " "
    except Exception as e:
        st.error(f"Error reading PDF: {e}")
    return text

def preprocess_text(text):
    """Cleans and preprocesses text for better similarity matching."""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

def calculate_similarity(resumes, job_desc):
    """Computes similarity scores between resumes and job description."""
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(resumes + [job_desc])
    similarity_scores = cosine_similarity(tfidf_matrix[:-1], tfidf_matrix[-1])
    return similarity_scores.flatten()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import PyPDF2
# import pandas as pd
# import numpy as np
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# from nltk.stem import WordNetLemmatizer
# from sentence_transformers import SentenceTransformer, util
# 
# # Download necessary NLTK data
# nltk.download("punkt")
# nltk.download("stopwords")
# nltk.download("wordnet")
# 
# # Function to extract text from PDF
# def extract_text_from_pdf(uploaded_file):
#     """Extracts text from a PDF file."""
#     text = ""
#     try:
#         pdf_reader = PyPDF2.PdfReader(uploaded_file)
#         for page in pdf_reader.pages:
#             text += page.extract_text() + " "
#     except Exception as e:
#         st.error(f"Error reading PDF: {e}")
#     return text
# 
# # Function to clean and preprocess text
# def preprocess_text(text):
#     """Cleans and preprocesses text for better similarity matching."""
#     text = text.lower()
#     text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
#     tokens = word_tokenize(text)
#     tokens = [word for word in tokens if word not in stopwords.words('english')]
#     lemmatizer = WordNetLemmatizer()
#     tokens = [lemmatizer.lemmatize(word) for word in tokens]
#     return ' '.join(tokens)
# 
# # Function to calculate similarity
# model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
# 
# def calculate_similarity(resumes, job_desc):
#     """Uses sentence embeddings to measure meaning-based similarity."""
#     all_texts = resumes + [job_desc]  # Combine resumes & job description
#     embeddings = model.encode(all_texts, convert_to_tensor=True)  # Convert text to embeddings
#     similarity_scores = util.pytorch_cos_sim(embeddings[:-1], embeddings[-1])  # Compare with job description
# 
#     return similarity_scores.squeeze().tolist()  # Return similarity scores
# 
# # Streamlit UI
# st.title("ðŸ“„ AI Resume Screening & Ranking System")
# 
# uploaded_files = st.file_uploader("Upload Resumes (Multiple PDFs allowed)", type=["pdf"], accept_multiple_files=True)
# job_description = st.text_area("Enter Job Description")
# 
# if st.button("Rank Resumes"):
#     if uploaded_files and job_description:
#         resumes_text = [extract_text_from_pdf(file) for file in uploaded_files]
#         resumes_cleaned = [preprocess_text(resume) for resume in resumes_text]
#         job_desc_cleaned = preprocess_text(job_description)
# 
#         similarity_scores = calculate_similarity(resumes_cleaned, job_desc_cleaned)
#         ranked_indices = np.argsort(similarity_scores)[::-1].tolist()  # Convert to a Python list
# 
#         results = pd.DataFrame({
#             'Resume': [uploaded_files[i].name for i in ranked_indices],  # Ensure index is an integer
#             'Similarity Score': [similarity_scores[i] for i in ranked_indices]  # Fix indexing issue
# })
# 
#         st.write("### ðŸ“Š Ranked Resumes:")
#         st.dataframe(results)
# 
#     else:
#         st.warning("âš  Please upload resumes and enter a job description.")
#

!pip install sentence-transformers --quiet

import nltk
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")

# Manually remove and re-download all tokenizers (Fixes `punkt_tab` error)
!rm -rf /root/nltk_data
nltk.download("all")

!rm -rf /root/.ngrok2/ngrok.yml
!rm -rf /root/.config/ngrok/ngrok.yml

!ngrok authtoken 2u9pUpbLRfVKXImZBYtu7Xa2zQa_2pX17Cia9xRKsCvMksMDW

!streamlit run app.py --server.port 8501 &>/dev/null &
# ngrok.kill()
# public_url = ngrok.connect(8501)
# print(f"ðŸš€ Open your app here: {public_url}")

from pyngrok import ngrok
ngrok.kill()  # Close old tunnels
public_url = ngrok.connect(8501)
print(f"ðŸš€ Open your app here: {public_url}")